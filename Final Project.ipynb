{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fca62dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "59074a74-9b21-416e-b870-504fb81d1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AI_Human.csv')\n",
    "\n",
    "# only keep normal characters\n",
    "cleaning_pattern = r\"[^a-zA-Z0-9\\s.,;:!?'\\\"()\\{\\}\\-—]\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')  # replace newline characters with space\n",
    "    text = re.sub(cleaning_pattern, '', text)  # remove unwanted characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # collapse multiple spaces into one\n",
    "    text = text.strip()  # remove leading/trailing spaces\n",
    "    return text\n",
    "    \n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df = df.drop_duplicates().reset_index(drop=True) # drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8335fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 4\n",
      "Max length: 1668\n",
      "Mean length: 393.05384024395755\n",
      "95th percentile length: 722\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of words in each text\n",
    "df['length'] = df['text'].str.split().apply(len)\n",
    "\n",
    "# Keep only sentences with more than 3 words\n",
    "df = df[df['length'] > 3].reset_index(drop=True)\n",
    "\n",
    "# Print basic length statistics\n",
    "print(\"Min length:\", df['length'].min())\n",
    "print(\"Max length:\", df['length'].max())\n",
    "print(\"Mean length:\", df['length'].mean())\n",
    "\n",
    "# Compute 95th percentile to determine max sequence length\n",
    "percentile_95 = int(np.percentile(df['length'], 95))\n",
    "print(\"95th percentile length:\", percentile_95)\n",
    "\n",
    "# Set maximum sequence length for vectorization\n",
    "max_len = percentile_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71c44cd0-b3f5-4dfc-a4d2-f63a585cc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "labels = df['generated']\n",
    "\n",
    "# split out train/test to test final model on\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "\n",
    "# split val set out from train to train model on\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "89a1ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 30000   # vocabulary size, counts tokens/words by frequency and any token not in the top max_tokens all become marked as the same\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52368558-3e7f-49cd-9062-14d74d7782b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">722</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">722</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d_5 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">722</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">722</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_5     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">722</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ spatial_dropout1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_pool_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ bidirectional_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionPool</span>)     │                   │            │ not_equal_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ attention_pool_5… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m722\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m722\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m3,840,000\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d_5 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m722\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_5         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m722\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_5     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m722\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m263,168\u001b[0m │ spatial_dropout1… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_pool_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ bidirectional_5[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionPool\u001b[0m)     │                   │            │ not_equal_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ attention_pool_5… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,729</span> (15.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,153,729\u001b[0m (15.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,217</span> (15.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,153,217\u001b[0m (15.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- configurable hyperparams ---\n",
    "max_tokens = max_tokens      # vocab size\n",
    "maxlen = max_len            # max sequence length\n",
    "embed_dim = 128\n",
    "rnn_units = 128\n",
    "dense_units = 128\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# --- small attention layer ---\n",
    "class AttentionPool(layers.Layer):\n",
    "    def __init__(self, units=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # tell Keras this layer understands masks\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.dense = layers.Dense(units, activation=\"tanh\")\n",
    "        self.score = layers.Dense(1, use_bias=False)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # This layer reduces the time dimension to a vector, so it produces no mask.\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # inputs: [B, T, F], mask: [B, T] or None\n",
    "        x = self.dense(inputs)            # [B, T, units]\n",
    "        scores = self.score(x)            # [B, T, 1]\n",
    "\n",
    "        if mask is not None:\n",
    "            # Broadcast mask to scores' shape and make masked positions very negative\n",
    "            mask = tf.cast(mask, scores.dtype)           # [B, T]\n",
    "            mask = tf.expand_dims(mask, axis=-1)         # [B, T, 1]\n",
    "            scores = scores + (mask - 1.0) * 1e9         # masked positions -> ~ -inf\n",
    "\n",
    "        weights = tf.nn.softmax(scores, axis=1)          # [B, T, 1]\n",
    "        weighted = tf.reduce_sum(weights * inputs, axis=1)  # [B, F]\n",
    "        return weighted\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"units\": self.dense.units if hasattr(self.dense, \"units\") else None})\n",
    "        return cfg\n",
    "\n",
    "# --- model ---\n",
    "inp = layers.Input(shape=(1,), dtype=tf.string)         # raw text\n",
    "x = vectorizer(inp)                                     # -> int sequence [B, T]\n",
    "x = layers.Embedding(input_dim=max_tokens, output_dim=embed_dim, mask_zero=True)(x)\n",
    "x = layers.SpatialDropout1D(dropout_rate)(x)\n",
    "x = layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True))(x)\n",
    "x = AttentionPool(units=64)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=out)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate),\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\", metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2867cfd6-5031-4ce5-901e-432fee76f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9716 - auc: 0.9917 - loss: 0.0791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 82ms/step - accuracy: 0.9880 - auc: 0.9984 - loss: 0.0363 - val_accuracy: 0.9947 - val_auc: 0.9992 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 82ms/step - accuracy: 0.9958 - auc: 0.9996 - loss: 0.0129 - val_accuracy: 0.9961 - val_auc: 0.9988 - val_loss: 0.0159 - learning_rate: 1.0000e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9970 - auc: 0.9998 - loss: 0.0095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 82ms/step - accuracy: 0.9973 - auc: 0.9998 - loss: 0.0082 - val_accuracy: 0.9980 - val_auc: 0.9998 - val_loss: 0.0053 - learning_rate: 1.0000e-04\n",
      "Epoch 4/5\n",
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 82ms/step - accuracy: 0.9980 - auc: 0.9999 - loss: 0.0059 - val_accuracy: 0.9985 - val_auc: 0.9997 - val_loss: 0.0048 - learning_rate: 1.0000e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9983 - auc: 0.9999 - loss: 0.0048"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5228/5228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 82ms/step - accuracy: 0.9985 - auc: 0.9999 - loss: 0.0042 - val_accuracy: 0.9985 - val_auc: 1.0000 - val_loss: 0.0039 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", patience=3, factor=0.5),\n",
    "    callbacks.ModelCheckpoint(\"best.h5\", monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n",
    "]\n",
    "\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "            .shuffle(10000)\n",
    "            .batch(64)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds = (tf.data.Dataset.from_tensor_slices((test_texts, test_labels))\n",
    "          .batch(64)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Train on GPU (ops created here will use GPU)\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=5,\n",
    "        callbacks=callbacks_list\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a34293db-30c4-4116-a5d4-80f8cce695d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.9985 - auc: 1.0000 - loss: 0.0039\n",
      "Test Loss: 0.0039\n",
      "Test Accuracy: 0.9985\n",
      "Test AUC: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_ds = (tf.data.Dataset.from_tensor_slices((test_texts, test_labels))\n",
    "           .batch(64)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "test_loss, test_accuracy, test_auc = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "03d1ce47-6559-452c-b812-044f3b15f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 31ms/step\n",
      "Confusion Matrix:\n",
      " [[198  90]\n",
      " [156 220]]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the test set\n",
    "pred_probs = model.predict(test_ds)\n",
    "pred_labels = (pred_probs >= 0.5).astype(int).flatten()  # convert to 0/1 labels\n",
    "\n",
    "# Get true labels (flatten if necessary)\n",
    "true_labels = np.array(test_labels)\n",
    "\n",
    "# confusion matrixcm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "abab7a8e-26ec-44eb-b981-307dab8a0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n",
      "Predicted probability: 1.0000\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# test string\n",
    "text_input = \"\"\"a\n",
    "\"\"\"\n",
    "text_input = clean_text(text_input)\n",
    "text_input = text_input.lower()\n",
    "\n",
    "# Convert to a batch of 1 element\n",
    "input_ds = tf.data.Dataset.from_tensor_slices([text_input]).batch(1)\n",
    "\n",
    "# Get predicted probability (between 0 and 1)\n",
    "pred_prob = model.predict(input_ds)[0][0]\n",
    "\n",
    "# Convert to class label (0 or 1)\n",
    "pred_label = int(pred_prob >= 0.5)\n",
    "\n",
    "print(f\"Predicted probability: {pred_prob:.4f}\")\n",
    "print(f\"Predicted label: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a7365d12-9a7d-4e93-9ab4-1614183befb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5654 - auc: 0.5000 - loss: nan\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.5654\n",
      "Test AUC: 0.5000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Confusion Matrix:\n",
      " [[  0 288]\n",
      " [  0 376]]\n"
     ]
    }
   ],
   "source": [
    "# I want to check predictions on data that isnt in this dataset to see if it can handle different data\n",
    "# found https://www.kaggle.com/datasets/prajwaldongre/llm-detect-ai-generated-vs-student-generated-text/data\n",
    "new_test_df = pd.read_csv('LLM.csv')\n",
    "new_test_df['Label'] = new_test_df['Label'].map({'ai':1, 'student':0})\n",
    "\n",
    "new_test_df['Text'] = new_test_df['Text'].apply(clean_text)\n",
    "new_test_df = new_test_df.drop_duplicates().reset_index(drop=True) # drop duplicates\n",
    "\n",
    "new_text = new_test_df['Text']\n",
    "new_label = new_test_df['Label']\n",
    "\n",
    "\n",
    "\n",
    "new_test_ds = (tf.data.Dataset.from_tensor_slices((new_text, new_label))\n",
    "           .batch(64)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "new_test_loss, new_test_accuracy, new_test_auc = model.evaluate(new_test_ds)\n",
    "print(f\"Test Loss: {new_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {new_test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {new_test_auc:.4f}\")\n",
    "\n",
    "# Get predictions for the test set\n",
    "new_pred_probs = model.predict(new_test_ds)\n",
    "new_pred_labels = (new_pred_probs >= 0.5).astype(int).flatten()  # convert to 0/1 labels\n",
    "\n",
    "# Get true labels (flatten if necessary)\n",
    "new_true_labels = np.array(new_label)\n",
    "\n",
    "# remove nan for bad predictions\n",
    "mask = ~np.isnan(new_label)\n",
    "new_true_labels = np.array(new_label)[mask]\n",
    "new_pred_labels = new_pred_labels[mask]\n",
    "\n",
    "# confusion matrix\n",
    "cm = confusion_matrix(new_true_labels, new_pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
