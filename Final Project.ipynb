{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fca62dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 19:13:31.761564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757704411.779235  157828 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757704411.785158  157828 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-12 19:13:31.803125: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59074a74-9b21-416e-b870-504fb81d1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('AI_Human.csv')\n",
    "\n",
    "# only keep normal characters\n",
    "cleaning_pattern = r\"[^a-zA-Z0-9\\s.,;:!?'\\\"()\\{\\}\\-—]\"\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')  # replace newline characters with space\n",
    "    text = re.sub(cleaning_pattern, ' ', text)  # remove unwanted characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # collapse multiple spaces into one\n",
    "    text = text.strip()  # remove leading/trailing spaces\n",
    "    return text\n",
    "    \n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df = df.drop_duplicates().reset_index(drop=True) # drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8335fd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size before cutoff: (464671, 3)\n",
      "df size after cutoff: (459120, 3)\n",
      "Min length: 140\n",
      "Max length: 1704\n",
      "Mean length: 396.951550792821\n",
      "95th percentile length: 725\n"
     ]
    }
   ],
   "source": [
    "# Compute the number of words in each text \n",
    "df['length'] = df['text'].str.split().apply(len) \n",
    "\n",
    "print('df size before cutoff: ' + str(df.shape))\n",
    "# Keep only essays with more than 3 words \n",
    "df = df[df['length'] >= 140].reset_index(drop=True) \n",
    "print('df size after cutoff: ' + str(df.shape))\n",
    "\n",
    "# Print basic length statistics \n",
    "print(\"Min length:\", df['length'].min()) \n",
    "print(\"Max length:\", df['length'].max()) \n",
    "print(\"Mean length:\", df['length'].mean()) \n",
    "\n",
    "# Compute 95th percentile to determine max sequence length \n",
    "percentile_95 = int(np.percentile(df['length'], 95)) \n",
    "print(\"95th percentile length:\", percentile_95) \n",
    "\n",
    "# Set maximum sequence length for vectorization \n",
    "max_len = percentile_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71c44cd0-b3f5-4dfc-a4d2-f63a585cc3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['text']\n",
    "labels = df['generated']\n",
    "\n",
    "# split out train/test to test final model on\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "\n",
    "# split val set out from train to train model on\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, stratify=train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a1ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757705335.364647  157828 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13294 MB memory:  -> device: 0, name: NVIDIA A2, pci bus id: 0000:00:10.0, compute capability: 8.6\n",
      "2025-09-12 19:29:07.967974: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 3146574456 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 30000   # vocabulary size, counts tokens/words by frequency and any token not in the top max_tokens all become marked as the same\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "vectorizer.adapt(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52368558-3e7f-49cd-9062-14d74d7782b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">725</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">725</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,840,000</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d_1 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">725</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">725</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_vectorizati… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">725</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> │ spatial_dropout1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_pool_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AttentionPool</span>)     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ attention_pool_1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ text_vectorization  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m725\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m725\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m3,840,000\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ spatial_dropout1d_1 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m725\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m725\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ text_vectorizati… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m725\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │    \u001b[38;5;34m263,168\u001b[0m │ spatial_dropout1… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_pool_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAttentionPool\u001b[0m)     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ attention_pool_1… │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,729</span> (15.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,153,729\u001b[0m (15.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,217</span> (15.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,153,217\u001b[0m (15.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- configurable hyperparams ---\n",
    "max_tokens = max_tokens      # vocab size for vectorizer, how many words it can know. 30000 is arbitrary, a real measure would be better.\n",
    "maxlen = max_len             # inputs are padded/truncated to this length, in this case 722 or the 95th percentile of word length balances too much truncation or large amounts of padding\n",
    "embed_dim = 128              # takes the vectorized data and maps it into this many dimensions to represent different features of a given word\n",
    "rnn_units = 128              # neurons in LSTM layer\n",
    "dense_units = 128            # neurons in dense layer\n",
    "dropout_rate = 0.3           # rate at which to drop features with dropout layers\n",
    "learning_rate = 1e-3         # steps at which to optimize loss\n",
    "\n",
    "# --- small attention pooling layer ---\n",
    "class AttentionPool(layers.Layer):\n",
    "    def __init__(self, units=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # tell Keras this layer understands masks\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.dense = layers.Dense(units, activation=\"tanh\")\n",
    "        self.score = layers.Dense(1, use_bias=False)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # This layer reduces the time dimension to a vector, so it produces no mask.\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # inputs: [B, T, F], mask: [B, T] or None\n",
    "        x = self.dense(inputs)            # [B, T, units]\n",
    "        scores = self.score(x)            # [B, T, 1]\n",
    "\n",
    "        if mask is not None:\n",
    "            # Broadcast mask to scores' shape and make masked positions very negative\n",
    "            mask = tf.cast(mask, scores.dtype)           # [B, T]\n",
    "            mask = tf.expand_dims(mask, axis=-1)         # [B, T, 1]\n",
    "            scores = scores + (mask - 1.0) * 1e9         # masked positions -> ~ -inf\n",
    "\n",
    "        weights = tf.nn.softmax(scores, axis=1)          # [B, T, 1]\n",
    "        weighted = tf.reduce_sum(weights * inputs, axis=1)  # [B, F]\n",
    "        return weighted\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"units\": self.dense.units if hasattr(self.dense, \"units\") else None})\n",
    "        return cfg\n",
    "\n",
    "# --- model architecture ---\n",
    "# raw text input\n",
    "# -> int sequence with max_len and max_tokens\n",
    "# -> Embedding layer embeds based on max_tokens, embed_dim, and sees 0 values from the vectorizer as padding\n",
    "# -> The Dropout layer will randomly 0 out values at the dropout rate to prevent over reliance on them, \n",
    "#    the 1d dropout layer does the same but with whole columns/dimensions from the embedding\n",
    "# -> Not Equal layer from mask_zero=True, creates a boolean mask of x != 0 to ensure the model isnt trying to learn from the padded layers\n",
    "# -> BiLSTM: more intensive than LSTM, might want to try just lstm\n",
    "#            Runs a rnn through the text backwards and forwards with the ability to weight how much of past information is forgot, remebered, and what is output\n",
    "# -> Attention Pooling: BiLSTM outputs data in the shape of [batch, timestamp, features] and outputs [batch, features], size 64 in this case, with the features being weighted based on learned importance of past words  \n",
    "#                       If all you do is take the last lstm state or word in the sequence you will lose potentially important past data\n",
    "#                       If you pool across mean/avg/min important information can be lost especially because of the length of the texts\n",
    "#                       AttentionPool class lets the model learn to weight various words in the seqeunce differently ex. the is less important than delve which ai loves\n",
    "# -> Batch Normalization takes in the [batch, weighted features] matrix from Attention pooling and normalizes the features in the given batch with mean ~0, sd ~1 \n",
    "#                        helps with speed and accuracy, allowing models to converge more efficently due to removing any changes between features and can reduce overfitting by adding a bit of noise\n",
    "# -> Dense layer: takes in the reduced size from the activation layer and transforms it into a larger(128) sized dimension, \n",
    "#                 uses a linear method to handle the still encoded, dense data, and a relu function to turn negative values to 0 removing linearity and allowing the model to learn more complex relationships\n",
    "# -> Dropout layer pt.2 but not 1d this time\n",
    "# -> final Dense layer with one node to give 0/1 predictions and a sigmoid to bring it between 0/1 for probabilities\n",
    "\n",
    "inp = layers.Input(shape=(1,), dtype=tf.string)         \n",
    "x = vectorizer(inp)                                     \n",
    "x = layers.Embedding(input_dim=max_tokens, output_dim=embed_dim, mask_zero=True)(x)\n",
    "x = layers.SpatialDropout1D(dropout_rate)(x)     \n",
    "x = layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True))(x)\n",
    "x = AttentionPool(units=64)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "x = layers.Dropout(dropout_rate)(x)\n",
    "out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = models.Model(inputs=inp, outputs=out)\n",
    "\n",
    "# --- model training ---\n",
    "# Adaptive moment estimation optimizer\n",
    "# momentum allows more efficent gradent descent by allwoing the updates to gain 'momentum' down the loss gradient \n",
    "# this accounts for gradients oscilating as they descend by canceling out oscilating gradients meaning smoother convergance and faster updates as the updates gain momentum allowing for bigger steps\n",
    "# Binary cross entropy: loss function for binary predictions that penalizes high predicted probabilities for true 0 and vice versa\n",
    "# accuracy just gives proportion of predictions matching true label\n",
    "# Area under curve: made for imbalanced data and measures area under true positve vs false positve ROC curve\n",
    "# allows for better accuracy measure on imbalanced data, .5 for random guesses and 1 for perfect predictions\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate),\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\", metrics.AUC(name=\"auc\")]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2867cfd6-5031-4ce5-901e-432fee76f4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    # stops training early if max auc on val set hasnt improved for 6 epochs and returns to model with the best auc\n",
    "    callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True),\n",
    "    # reduces learning rate by half if max val auc hasnt improved for 3 epochs\n",
    "    callbacks.ReduceLROnPlateau(monitor=\"val_auc\", mode=\"max\", patience=3, factor=0.5),\n",
    "    # every epoch if there is a new max val auc, save the model to best.keras\n",
    "    callbacks.ModelCheckpoint(\"best.keras\", monitor=\"val_auc\", mode=\"max\", save_best_only=True)\n",
    "]\n",
    "\n",
    "\n",
    "train_ds = (tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "            .shuffle(10000) # shuffles data before passing it to the model to make sure the model isnt seeing it in the same order every time\n",
    "            .batch(64) # passes data to model in batches for gpu multi processing\n",
    "            .prefetch(tf.data.AUTOTUNE)) # speeds up model processing by multi threading data fetching and model training\n",
    "\n",
    "val_ds = (tf.data.Dataset.from_tensor_slices((test_texts, test_labels))\n",
    "          .batch(64)\n",
    "          .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Train on GPU )\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=3,\n",
    "        callbacks=callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a34293db-30c4-4116-a5d4-80f8cce695d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.9976 - auc: 0.9997 - loss: 0.0075\n",
      "Test Loss: 0.0075\n",
      "Test Accuracy: 0.9976\n",
      "Test AUC: 0.9997\n"
     ]
    }
   ],
   "source": [
    "test_ds = (tf.data.Dataset.from_tensor_slices((test_texts, test_labels))\n",
    "           .batch(64)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# load model in\n",
    "# model = tf.keras.models.load_model(\"best.keras\", custom_objects={\"AttentionPool\": AttentionPool})\n",
    "\n",
    "\n",
    "test_loss, test_accuracy, test_auc = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03d1ce47-6559-452c-b812-044f3b15f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m718/718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step\n",
      "Confusion Matrix:\n",
      " [[28458    12]\n",
      " [  100 17342]]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the test set\n",
    "pred_probs = model.predict(test_ds)\n",
    "pred_labels = (pred_probs >= 0.5).astype(int).flatten()  # convert to 0/1 labels\n",
    "\n",
    "# Get true labels (flatten if necessary)\n",
    "true_labels = np.array(test_labels)\n",
    "\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abab7a8e-26ec-44eb-b981-307dab8a0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "Predicted probability: 1.0000\n",
      "Predicted label: 1\n"
     ]
    }
   ],
   "source": [
    "# test string\n",
    "text_input = \"\"\"a\n",
    "\"\"\"\n",
    "text_input = clean_text(text_input)\n",
    "text_input = text_input.lower()\n",
    "\n",
    "# Convert to a batch of 1 element\n",
    "input_ds = tf.data.Dataset.from_tensor_slices([text_input]).batch(1)\n",
    "\n",
    "# Get predicted probability (between 0 and 1)\n",
    "pred_prob = model.predict(input_ds)[0][0]\n",
    "\n",
    "# Convert to class label (0 or 1)\n",
    "pred_label = int(pred_prob >= 0.5)\n",
    "\n",
    "print(f\"Predicted probability: {pred_prob:.4f}\")\n",
    "print(f\"Predicted label: {pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7365d12-9a7d-4e93-9ab4-1614183befb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5654 - auc: 0.5000 - loss: nan\n",
      "Test Loss: nan\n",
      "Test Accuracy: 0.5654\n",
      "Test AUC: 0.5000\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Confusion Matrix:\n",
      " [[  0 288]\n",
      " [  0 376]]\n"
     ]
    }
   ],
   "source": [
    "# I want to check predictions on data that isnt in this dataset to see if it can handle different data\n",
    "# found https://www.kaggle.com/datasets/prajwaldongre/llm-detect-ai-generated-vs-student-generated-text/data\n",
    "new_test_df = pd.read_csv('LLM.csv')\n",
    "new_test_df['Label'] = new_test_df['Label'].map({'ai':1, 'student':0})\n",
    "\n",
    "new_test_df['Text'] = new_test_df['Text'].apply(clean_text)\n",
    "new_test_df = new_test_df.drop_duplicates().reset_index(drop=True) # drop duplicates\n",
    "\n",
    "new_text = new_test_df['Text']\n",
    "new_label = new_test_df['Label']\n",
    "\n",
    "\n",
    "\n",
    "new_test_ds = (tf.data.Dataset.from_tensor_slices((new_text, new_label))\n",
    "           .batch(64)\n",
    "           .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "new_test_loss, new_test_accuracy, new_test_auc = model.evaluate(new_test_ds)\n",
    "print(f\"Test Loss: {new_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {new_test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {new_test_auc:.4f}\")\n",
    "\n",
    "# Get predictions for the test set\n",
    "new_pred_probs = model.predict(new_test_ds)\n",
    "new_pred_labels = (new_pred_probs >= 0.5).astype(int).flatten()  # convert to 0/1 labels\n",
    "\n",
    "# Get true labels (flatten if necessary)\n",
    "new_true_labels = np.array(new_label)\n",
    "\n",
    "# remove nan for bad predictions\n",
    "mask = ~np.isnan(new_label)\n",
    "new_true_labels = np.array(new_label)[mask]\n",
    "new_pred_labels = new_pred_labels[mask]\n",
    "\n",
    "# confusion matrix\n",
    "new_cm = confusion_matrix(new_true_labels, new_pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", new_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
