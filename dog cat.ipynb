{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fa07cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import ydf\n",
    "from keras.applications import MobileNetV2 # Using a lightweight model suitable for 64x64 images\n",
    "\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oops\n",
      "Corrupted images found: 1\n",
      "['PetImages/Dog\\\\9041.jpg']\n"
     ]
    }
   ],
   "source": [
    "bad_images = []\n",
    "for subdir, _, files in os.walk(\"PetImages/\"):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg')):\n",
    "            img_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                        # handle userwarnings as errors with warnings package\n",
    "                        warnings.simplefilter('error', UserWarning)\n",
    "                        img = Image.open(img_path)\n",
    "                        img.verify()  # Check if image is corrupted\n",
    "            except:\n",
    "                print('oops')\n",
    "                bad_images.append(img_path)\n",
    "\n",
    "print(\"Corrupted images found:\", len(bad_images))\n",
    "print(bad_images) #['PetImages/Cat\\\\666.jpg', 'PetImages/Dog\\\\11702.jpg'] ['PetImages/Dog\\\\9041.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf0c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorFlow-based image verification...\n",
      "Corrupt file found (TF): PetImages\\Cat\\10404.jpg\n",
      "Corrupt file found (TF): PetImages\\Cat\\4351.jpg\n",
      "Corrupt file found (TF): PetImages\\Cat\\Thumbs.db\n",
      "Corrupt file found (TF): PetImages\\Dog\\11233.jpg\n",
      "Corrupt file found (TF): PetImages\\Dog\\11912.jpg\n",
      "Corrupt file found (TF): PetImages\\Dog\\2317.jpg\n",
      "Corrupt file found (TF): PetImages\\Dog\\2494.jpg\n",
      "Corrupt file found (TF): PetImages\\Dog\\9500.jpg\n",
      "Corrupt file found (TF): PetImages\\Dog\\Thumbs.db\n",
      "\n",
      "---------------------------------\n",
      "Total corrupted or problematic images found: 9\n",
      "['PetImages\\\\Cat\\\\10404.jpg', 'PetImages\\\\Cat\\\\4351.jpg', 'PetImages\\\\Cat\\\\Thumbs.db', 'PetImages\\\\Dog\\\\11233.jpg', 'PetImages\\\\Dog\\\\11912.jpg', 'PetImages\\\\Dog\\\\2317.jpg', 'PetImages\\\\Dog\\\\2494.jpg', 'PetImages\\\\Dog\\\\9500.jpg', 'PetImages\\\\Dog\\\\Thumbs.db']\n",
      "\n",
      "Deleting corrupted files...\n",
      "Deleted: PetImages\\Cat\\10404.jpg\n",
      "Deleted: PetImages\\Cat\\4351.jpg\n",
      "Deleted: PetImages\\Cat\\Thumbs.db\n",
      "Deleted: PetImages\\Dog\\11233.jpg\n",
      "Deleted: PetImages\\Dog\\11912.jpg\n",
      "Deleted: PetImages\\Dog\\2317.jpg\n",
      "Deleted: PetImages\\Dog\\2494.jpg\n",
      "Deleted: PetImages\\Dog\\9500.jpg\n",
      "Deleted: PetImages\\Dog\\Thumbs.db\n",
      "Deletion complete.\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"PetImages\"\n",
    "subfolders = [\"Cat\", \"Dog\"]\n",
    "bad_images = []\n",
    "\n",
    "print(\"Starting TensorFlow-based image verification...\")\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    folder_path = os.path.join(image_folder, subfolder)\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Directory not found: {folder_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # We need to use a try-except block here\n",
    "        try:\n",
    "            # Read the raw file data\n",
    "            raw_image_data = tf.io.read_file(file_path)\n",
    "            \n",
    "            # Attempt to decode it using TensorFlow's engine\n",
    "            # This is the same operation that fails during training\n",
    "            tf.io.decode_image(raw_image_data)\n",
    "            \n",
    "        except tf.errors.InvalidArgumentError:\n",
    "            # This is the specific error we are looking for!\n",
    "            print(f\"Corrupt file found (TF): {file_path}\")\n",
    "            bad_images.append(file_path)\n",
    "        except Exception as e:\n",
    "            # Catch any other potential errors, e.g., file not readable\n",
    "            print(f\"An unexpected error occurred for file {file_path}: {e}\")\n",
    "            bad_images.append(file_path)\n",
    "\n",
    "print(\"\\n---------------------------------\")\n",
    "print(f\"Total corrupted or problematic images found: {len(bad_images)}\")\n",
    "print(bad_images)\n",
    "\n",
    "# --- Optional: Automatically delete the bad files ---\n",
    "# Uncomment the following lines to delete the files found\n",
    "print(\"\\nDeleting corrupted files...\")\n",
    "for fpath in bad_images:\n",
    "    try:\n",
    "        os.remove(fpath)\n",
    "        print(f\"Deleted: {fpath}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error deleting {fpath}: {e}\")\n",
    "print(\"Deletion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "debba975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24990 files belonging to 2 classes.\n",
      "Using 19992 files for training.\n",
      "Found 24990 files belonging to 2 classes.\n",
      "Using 4998 files for validation.\n",
      "Class names found: ['Cat', 'Dog']\n"
     ]
    }
   ],
   "source": [
    "image_folder = \"PetImages/\"\n",
    "img_size = (64, 64)\n",
    "batch_size = 32\n",
    "\n",
    "# Create a training dataset directly from the directory\n",
    "# It will automatically infer class labels from the folder names\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    image_folder,\n",
    "    validation_split=0.2,  # Use 20% of the images for validation\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create a validation dataset\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    image_folder,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Get class names (e.g., ['Cat', 'Dog'])\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Class names found:\", class_names)\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "# normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "# train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "# validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f581c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.125..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [3.0078125..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [2.7832031..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..254.95312].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.65625..255.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.17578125..252.50781].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.061523438..225.22388].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [11.9592285..251.43872].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [14.1875..232.3125].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Take one batch from the dataset\n",
    "# A batch contains a tensor of images and a tensor of their corresponding labels\n",
    "for images, labels in train_dataset.take(1):\n",
    "  # `images` is a batch of images (e.g., 32 images of shape 64x64x3)\n",
    "  # `labels` is a batch of labels (e.g., 32 integers from 0 to n_classes-1)\n",
    "  \n",
    "  # Loop through the first 9 images in the batch\n",
    "  for i in range(9):\n",
    "    # Create a subplot for each image\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    \n",
    "    # Display the image\n",
    "    # Note: Keras reads images as tf.Tensor, matplotlib expects NumPy arrays.\n",
    "    # We also expect pixel values to be in the range [0, 1] for floating point images.\n",
    "    # If you haven't normalized yet, you'd do images[i].numpy().astype(\"uint8\")\n",
    "    plt.imshow(images[i].numpy().astype(\"float32\"))\n",
    "    \n",
    "    # Get the corresponding label name from the class_names list\n",
    "    label_name = class_names[labels[i]]\n",
    "    plt.title(label_name)\n",
    "    \n",
    "    # Hide the axes\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac12a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ligma\\AppData\\Local\\Temp\\ipykernel_59492\\358037125.py:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  feature_extractor = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Extracting features...\n",
      "\n",
      "Shape of training features: (19992, 1280)\n",
      "Shape of validation features: (4998, 1280)\n",
      "\n",
      "Training DataFrame head:\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0   0.000000   0.000000   0.013471   1.711313   0.000000   0.000000   \n",
      "1   0.000000   0.211355   1.396279   1.236521   0.175555   0.000000   \n",
      "2   0.000000   0.462304   0.000000   0.923774   6.000000   0.000000   \n",
      "3   1.537643   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
      "4   0.132699   0.000000   3.005672   0.000000   0.000000   2.306738   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_1271  \\\n",
      "0   0.000000   0.000000   0.000000   0.034662  ...      0.000000   \n",
      "1   0.000000   0.000000   6.000000   0.000000  ...      0.010942   \n",
      "2   1.527553   0.881599   5.669243   2.504699  ...      0.000000   \n",
      "3   0.000000   0.000000   0.000000   0.000000  ...      0.194643   \n",
      "4   6.000000   0.458991   5.774117   0.000000  ...      0.000000   \n",
      "\n",
      "   feature_1272  feature_1273  feature_1274  feature_1275  feature_1276  \\\n",
      "0      0.139685      0.000000      4.058568      0.000000      1.150715   \n",
      "1      0.183817      1.462158      0.000000      0.001911      1.297404   \n",
      "2      0.000000      0.000000      2.110206      2.320620      5.393417   \n",
      "3      0.000000      0.000000      0.000000      2.446881      3.950929   \n",
      "4      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
      "\n",
      "   feature_1277  feature_1278  feature_1279  label  \n",
      "0       0.00000      0.000000      0.000000      1  \n",
      "1       2.90869      0.000000      0.121087      0  \n",
      "2       6.00000      5.529515      2.952203      1  \n",
      "3       0.00000      0.068054      0.000000      1  \n",
      "4       0.00000      2.790040      0.000000      1  \n",
      "\n",
      "[5 rows x 1281 columns]\n",
      "\n",
      "Training YDF Random Forest model...\n",
      "Train model on 19992 examples\n",
      "Model trained in 0:00:21.208856\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Evaluation Report:\n",
      "accuracy: 0.822529\n",
      "confusion matrix:\n",
      "    label (row) \\ prediction (col)\n",
      "    +------+------+------+\n",
      "    |      |    0 |    1 |\n",
      "    +------+------+------+\n",
      "    |    0 | 2193 |  249 |\n",
      "    +------+------+------+\n",
      "    |    1 |  638 | 1918 |\n",
      "    +------+------+------+\n",
      "characteristics:\n",
      "    name: '1' vs others\n",
      "    ROC AUC: 0.912658\n",
      "    PR AUC: 0.923897\n",
      "    Num thresholds: 302\n",
      "loss: 0.398958\n",
      "num examples: 4998\n",
      "num examples (weighted): 4998\n",
      "\n",
      "\n",
      "Model Accuracy on Validation Set: 82.25%\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,        # Exclude the final classification layer\n",
    "    pooling='avg',            # Average the features into a single vector per image\n",
    "    input_shape=(*img_size, 3)\n",
    ")\n",
    "\n",
    "# --- 3. Define a Function to Extract Features from a Dataset ---\n",
    "def extract_features(dataset, extractor_model):\n",
    "    \"\"\"Iterates through a tf.data.Dataset, extracts features, and returns them as NumPy arrays.\"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Extracting features...\")\n",
    "    # Iterate through each batch of images and labels in the dataset\n",
    "    for images, labels in dataset:\n",
    "        # Use the specific preprocessing function for MobileNetV2\n",
    "        preprocessed_images = tf.keras.applications.mobilenet_v2.preprocess_input(images)\n",
    "        \n",
    "        # Get feature vectors from the extractor model\n",
    "        features = extractor_model.predict(preprocessed_images, verbose=0)\n",
    "        \n",
    "        # Store the results from the batch\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "    # Concatenate all batches into single NumPy arrays\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "# --- 4. Process Both Datasets ---\n",
    "train_features, train_labels = extract_features(train_dataset, feature_extractor)\n",
    "val_features, val_labels = extract_features(validation_dataset, feature_extractor)\n",
    "\n",
    "print(f\"\\nShape of training features: {train_features.shape}\")\n",
    "print(f\"Shape of validation features: {val_features.shape}\")\n",
    "\n",
    "# --- 5. Convert to Pandas DataFrames for YDF ---\n",
    "# Create column names for the features\n",
    "feature_columns = [f'feature_{i}' for i in range(train_features.shape[1])]\n",
    "\n",
    "# Create the training DataFrame\n",
    "train_df = pd.DataFrame(train_features, columns=feature_columns)\n",
    "train_df['label'] = train_labels\n",
    "\n",
    "# Create the validation DataFrame (which we'll use for testing)\n",
    "test_df = pd.DataFrame(val_features, columns=feature_columns)\n",
    "test_df['label'] = val_labels\n",
    "\n",
    "print(\"\\nTraining DataFrame head:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# --- 6. Train and Evaluate the YDF Random Forest Model ---\n",
    "print(\"\\nTraining YDF Random Forest model...\")\n",
    "\n",
    "# Define the model. YDF will automatically detect the classification task.\n",
    "rf_model = ydf.RandomForestLearner(label=\"label\").train(train_df)\n",
    "\n",
    "# Evaluate the model's performance on the validation data\n",
    "print(\"\\nEvaluating model performance...\")\n",
    "evaluation = rf_model.evaluate(test_df)\n",
    "\n",
    "print(\"\\nEvaluation Report:\")\n",
    "print(evaluation)\n",
    "\n",
    "accuracy = evaluation.accuracy\n",
    "print(f\"\\nModel Accuracy on Validation Set: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
